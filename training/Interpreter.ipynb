{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Interpreter.ipynb","private_outputs":true,"provenance":[{"file_id":"1x3tby9J00egepof5936ik6HcUhMoBspu","timestamp":1586548024302}],"collapsed_sections":["ePYKUUMTKyUn","0RbKhiS9lJR-","s25qd-xQj6XD","oYd6vLA45fzB","DDJY_VuwkRw2","kInDbyrN9lhr"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ePYKUUMTKyUn"},"source":["# Setup enviroment"]},{"cell_type":"code","metadata":{"id":"QqCgg6b9OIgH"},"source":["!pip3 install hiddenlayer > /dev/null\n","!pip3 install ipdb > /dev/null\n","import json\n","import os\n","import numpy as np\n","import random\n","from datetime import datetime as dt\n","\n","import torch as tr\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from torchvision.datasets.folder import default_loader\n","from torchvision.transforms import functional as ft\n","from torchvision import transforms\n","from PIL import Image, ImageDraw, ImageFilter\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import balanced_accuracy_score\n","\n","import matplotlib\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import Normalize\n","import hiddenlayer as hl\n","\n","from google.colab import drive\n","\n","from tqdm.notebook import tqdm\n","\n","tr.backends.cudnn.deterministic = False\n","tr.backends.cudnn.benchmark = True\n","tr.manual_seed(42)\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gubZcgYnOnMF"},"source":["drive.mount('./drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huS-FFXzOppO"},"source":["os.chdir(\"/content/drive/My Drive/Workspace/pic2sgf/training\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RbKhiS9lJR-"},"source":["# Board extractor"]},{"cell_type":"code","metadata":{"id":"8UJcj0RMlLpV"},"source":["class BoardExtractor():\n","    def __init__(self, board_size):\n","        self.size = board_size * 24\n","        B = np.array([[8, 8, 1],\n","                      [self.size-8, 8, 1],\n","                      [self.size-8, self.size-8, 1],\n","                      [8, self.size-8, 1]]).T\n","        self.T2 = np.linalg.inv(B[:, 0:3] * np.linalg.solve(B[:, 0:3], B[:, 3]))\n","\n","    def __call__(self, img, vertexs):\n","        A = np.concatenate([vertexs.T, np.array([[1.0, 1.0, 1.0, 1.0]])], axis=0)\n","        T1 = A[:, 0:3] * np.linalg.solve(A[:, 0:3], A[:, 3])\n","        T = np.matmul(T1, self.T2)\n","        T /= T[2,2]\n","        board = img.transform((self.size, self.size),\n","                              method=Image.PERSPECTIVE,\n","                              data = T.reshape(-1),\n","                              resample=Image.BILINEAR)\n","        return board.transpose(Image.ROTATE_180), T[2,0:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hpHGqQXgEJe"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"ezH-NmKKIC07"},"source":["class BoardPositionDataset(Dataset):\n","    def __init__(self, metadata_files, size, augment=True, create_wrong=True, only_labelled=True):\n","        super(BoardPositionDataset, self).__init__()\n","        self.board_extractor = BoardExtractor(size)\n","        self.augment_images = augment\n","        self.create_wrong = create_wrong\n","        self.only_labelled = only_labelled\n","        self.displacement_scale = 4\n","        self.images = []\n","        self.position = {}\n","        self.corners = []\n","        self.orig_corners = []\n","        self.filename = []\n","        self.size = []\n","        self.upsampler = nn.Upsample(size, mode='bilinear', align_corners=True)\n","\n","        for file in metadata_files:\n","            print(f'Loading {file}...')\n","            with open(file, 'r') as f:\n","                metadata = json.load(f)\n","            for entry in tqdm(metadata):\n","                if int(entry['size']) != size: continue\n","                if self.only_labelled and ('positions' not in entry): continue\n","                self.size.append(entry['size'])\n","                \n","                img = self.load_image(\"images/\" + entry['filename'])\n","                self.orig_corners.append(entry['corners'])\n","                corner = np.array(entry['corners'])/100\n","                corner[:, 0] *= img.size[0]\n","                corner[:, 1] *= img.size[1]\n","                \n","                self.corners.append(self.order_vertexs(corner, img.size))\n","                self.filename.append(entry['filename'])\n","                self.images.append(img)\n","                if 'positions' in entry:\n","                    self.position[len(self.images) - 1] = tr.LongTensor(entry['positions']).reshape(size, size).permute(1,0).flip(1) + 1\n","\n","    def load_image(self, filename):\n","        img = default_loader(filename)\n","        if img.size[0] > img.size[1]:\n","            img = img.resize((512, 384), resample=Image.BILINEAR)\n","        else:\n","            img = img.resize((384, 512), resample=Image.BILINEAR).transpose(Image.ROTATE_90)\n","        return img\n","\n","\n","    def augment(self, img, pos, dis):\n","        if self.augment_images:\n","            img = ft.adjust_brightness(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","            img = ft.adjust_contrast(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","            img = ft.adjust_gamma(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","            img = ft.adjust_saturation(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","\n","            if np.random.binomial(1, 0.2): \n","                img = img.filter(ImageFilter.GaussianBlur(radius = np.clip(np.random.normal(loc=0.5, scale=0.25), 0, 1)))\n","\n","            if np.random.binomial(1, 0.5): \n","                img, dis = ft.hflip(img), dis.flip(2)\n","                if pos is not None:\n","                    pos = pos.flip(1)\n","                dis[0,:,:] *= -1\n","        return img, pos, dis\n","\n","    def order_vertexs(self, v, img_size):\n","        w, h = img_size\n","        vc = v.copy()\n","        idxs = np.ones(4).astype(int)\n","        idxs[0] = np.linalg.norm(vc, ord=2, axis=1).argmin()\n","        vc[idxs[0]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[1] = np.linalg.norm(vc - np.array([w,0]), ord=2, axis=1).argmin()\n","        vc[idxs[1]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[2] = np.linalg.norm(vc - np.array([w,h]), ord=2, axis=1).argmin()\n","        vc[idxs[2]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[3] = np.linalg.norm(vc - np.array([0,h]), ord=2, axis=1).argmin()\n","        vc[idxs[3]] = np.array([float('inf'), float('inf')])\n","\n","        last_prod = 0\n","        for i in range(len(v)):\n","            prev = v[idxs[(i-1)%4]] - v[idxs[i]]\n","            post = v[idxs[(i+1)%4]] - v[idxs[i]]\n","            cross_prod = np.cross(post, prev)\n","            if cross_prod * last_prod < 0:\n","                idxs[i], idxs[(i+1)%4] = idxs[(i+1)%4].copy(), idxs[i].copy()\n","                i += 1\n","            else:\n","                last_prod = cross_prod\n","        return v[idxs]\n","\n","    def __len__(self):\n","      return len(self.images)\n","\n","    def __getitem__(self, i):\n","        corner = np.copy(self.corners[i])\n","        wrong = 0\n","        displacement = np.random.normal(loc=0, scale=self.displacement_scale, size=(4,2))\n","        if self.augment_images:\n","            corner += displacement\n","        if self.create_wrong and np.random.binomial(1, 0.2):\n","            wrong = 1\n","            idx = np.random.randint(0, 4)\n","            corner[idx] += np.random.randint(12, 100, size=2) * (2*np.random.binomial([0,1], 0.5)-1)\n","        image, _ = self.board_extractor(self.images[i], corner)\n","\n","        displacement = tr.stack([tr.Tensor(displacement[[2,3,1,0], 0].reshape(2, 2)),\n","                                tr.Tensor(displacement[[2,3,1,0], 1].reshape(2, 2))], dim=0)\n","        displacement = self.upsampler(displacement.unsqueeze(0)).squeeze()\n","\n","        position = self.position[i].clone() if i in self.position else None\n","        if self.augment_images:\n","            image, position, displacement = self.augment(image, position, displacement)\n","        return (\n","            ft.to_tensor(image),\n","            position,\n","            displacement,\n","            tr.Tensor([wrong])\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G63CrCiOC1DM"},"source":["dataset09 = BoardPositionDataset(['metadata.json', 'self_labels.json'], 9)\n","dataset13 = BoardPositionDataset(['metadata.json', 'self_labels.json'], 13)\n","dataset19 = BoardPositionDataset(['metadata.json', 'self_labels.json'], 19)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMQVcc7yAf7E"},"source":["## Plots"]},{"cell_type":"code","metadata":{"id":"ooW6MN0n35dm"},"source":["dataset = dataset09\n","\n","plt.figure(figsize=(16, 16))\n","for i in range(9):\n","    plt.subplot(3, 3, i+1)\n","    index = random.randrange(len(dataset))\n","    img, _, disp, _ = dataset[index]\n","    plt.imshow(img.permute(1, 2, 0),\n","               extent=(0,1,0,1))\n","    plt.imshow(disp[0,:,:],\n","               cmap='seismic',\n","               extent=(0,1,0,1),\n","               norm = Normalize(vmin = -10, vmax = 10),\n","               alpha=0.4)\n","    plt.title(dataset.filename[index])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dAZuGb92Iuk"},"source":["dataset = dataset09\n","\n","plt.figure(figsize=(16, 16))\n","for i in range(9):\n","    plt.subplot(3, 3, i+1)\n","    index = random.randrange(len(dataset))\n","    img, bpos, _, _ = dataset[index]\n","    plt.imshow(img.permute(1, 2, 0),\n","               extent=(0, 1, 0, 1))\n","    plt.imshow(bpos,\n","               norm = Normalize(vmin = 0, vmax = 2),\n","               cmap='bwr',\n","               extent=(0,1,0,1),\n","               alpha=0.2)\n","    plt.title(dataset.filename[index])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s25qd-xQj6XD"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"5Nt5X_74oFqh"},"source":["class iblock(nn.Module):\n","    def __init__(self, dims):\n","        super(iblock, self).__init__()\n","        self.conv_path = nn.Sequential(nn.BatchNorm2d(dims), nn.GELU(), nn.Dropout2d(0.1),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1),\n","                                       nn.GELU(), nn.BatchNorm2d(dims),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1))\n","        \n","    def forward(self, x):\n","        return x + self.conv_path(x)\n","\n","\n","def Pooling(in_dim, out_dim):\n","    return nn.Sequential(nn.BatchNorm2d(in_dim), nn.GELU(),\n","                         nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),\n","                         nn.MaxPool2d(2))\n","\n","\n","class Interpreter(nn.Module):\n","    def __init__(self):\n","        super(Interpreter, self).__init__()\n","        self.conv_blocks = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=3),\n","            nn.GELU(), nn.BatchNorm2d(32),\n","            nn.Conv2d(32, 12, kernel_size=3, padding=1),\n","            iblock(12), iblock(12), iblock(12), iblock(12),\n","            Pooling(12, 24), \n","            iblock(24), iblock(24), iblock(24), iblock(24),\n","            Pooling(24, 48), \n","            iblock(48), iblock(48), iblock(48), iblock(48),\n","            Pooling(48, 96),\n","            iblock(96), iblock(96), iblock(96), iblock(96),\n","            nn.GELU(), nn.BatchNorm2d(96),\n","            )\n","        self.displacement = nn.Sequential(\n","            nn.Conv2d(96, 24, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(24),\n","            nn.Conv2d(24, 2, kernel_size=1)\n","        )\n","        self.position = nn.Sequential(\n","            nn.Conv2d(96, 24, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(24),\n","            nn.Conv2d(24, 3, kernel_size=1)\n","        )\n","        self.wrong = nn.Sequential(\n","            nn.Conv2d(96, 24, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(24),\n","            nn.Conv2d(24, 1, kernel_size=1),\n","            nn.AdaptiveMaxPool2d(1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_blocks(x)\n","        position = self.position(x)\n","        displacement = self.displacement(x)\n","        wrong = self.wrong(x).view(-1, 1)\n","        return position, displacement, wrong\n","      \n","    def load(self, fname):\n","        self.load_state_dict(tr.load(fname, map_location=lambda storage, loc: storage))\n","\n","    def save(self, fname):\n","        tr.save(self.state_dict(), fname)\n","\n","print(sum(p.numel() for p in Interpreter().parameters()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYd6vLA45fzB"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"DXwJunSVfs4V"},"source":["nc = [0.0,0.0,0.0]\n","for ds in [dataset09, dataset13, dataset19]:\n","    for _, pos, _, _ in ds:\n","        for i in range(3):\n","            nc[i] += (pos==i).sum()\n","smooth = 5000\n","tot = sum(nc) + 2 * smooth\n","nc[0] += smooth\n","nc[2] += smooth\n","class_weight = tr.Tensor([tot / nc[i] for i in range(3)]).cuda()\n","class_weight /= class_weight.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqQyKK9ugkOG"},"source":["def acc(pos, pred):\n","    pos = pos.cpu().detach().numpy()\n","    pred= pred.cpu().detach().numpy().argmax(axis=1)\n","    res = balanced_accuracy_score(pos.reshape(-1), pred.reshape(-1))\n","    return res\n","\n","def fit(model, data_loader, optimizer, class_weight, backward=True):\n","    gtrue = []\n","    preds = []\n","    wtrue = []\n","    wpred = []\n","    total_loss = 0\n","    total_disp_loss = 0\n","    if backward: model.train()\n","    else: model.eval()\n","    for img, pos, disp, wr in data_loader:\n","        img, pos, disp, wr = img.cuda(), pos.cuda(), disp.cuda(), wr.cuda()\n","        ppos, pdisp, pwr = model(img)\n","\n","        if backward: optimizer.zero_grad()\n","        correct = (wr < 0.5).squeeze(1)\n","        pos_loss  = nn.functional.cross_entropy(ppos[correct], pos[correct], weight=class_weight)\n","        disp_loss = nn.functional.mse_loss(pdisp[correct], disp[correct])\n","        wrong_loss = nn.functional.binary_cross_entropy(pwr, wr)\n","        loss = pos_loss + wrong_loss + 0.1*disp_loss\n","        if backward: loss.backward()\n","        if backward: optimizer.step()\n","\n","        gtrue.append( pos[correct].detach())\n","        preds.append(ppos[correct].detach())\n","        wtrue.append( wr.detach())\n","        wpred.append(pwr.detach())\n","\n","        total_loss += (pos_loss + wrong_loss).data.item() / len(data_loader)\n","        total_disp_loss += disp_loss.mean().data.item() / len(data_loader)\n","    pos_acc = acc(tr.cat(gtrue).cpu(), tr.cat(preds).cpu())\n","    wrong_acc = balanced_accuracy_score(tr.cat(wtrue).cpu() > 0.5, tr.cat(wpred).cpu() > 0.5)\n","    return total_loss, pos_acc, total_disp_loss, wrong_acc\n","\n","def evaluate_model(model, data_loader, class_weight):\n","    return fit(model, data_loader, optimizer, class_weight, backward=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGpuoL42l4jk"},"source":["model = Interpreter().cuda()\n","tr.manual_seed(42)\n","np.random.seed(42)\n","\n","fname ='interpreter_32'\n","# optimizer = tr.optim.Adam(model.parameters(), lr=0.002, weight_decay=1e-5)\n","# scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n","optimizer = tr.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOhFPqxf5eqs"},"source":["hist = hl.History()\n","canvas = hl.Canvas()\n","\n","if os.path.isfile(\"models/\" + fname + \".pmt\"):\n","    model.load(\"models/\" + fname + \".pmt\")\n","    hist.load(\"models/\" + fname + \".hist\")\n","    epoch = len(hist['best'].data) + 1\n","    mean_best = min(hist['best'].data)\n","else:\n","    epoch = 0\n","    mean_best = float('inf')\n","\n","dataset09.augment_images = True\n","dataset13.augment_images = True\n","dataset19.augment_images = True\n","\n","train09_loader = DataLoader(dataset09, batch_size=12, shuffle=True)\n","train13_loader = DataLoader(dataset13, batch_size=12, shuffle=True)\n","train19_loader = DataLoader(dataset19, batch_size=12, shuffle=True)\n","      \n","epochs_without_improvement = 0  \n","while epochs_without_improvement < 200:\n","    train19_loss, train19_acc, train19_dloss, train19_wacc = fit(model, train19_loader, optimizer, class_weight)\n","    train09_loss, train09_acc, train09_dloss, train09_wacc = fit(model, train09_loader, optimizer, class_weight)\n","    train13_loss, train13_acc, train13_dloss, train13_wacc = fit(model, train13_loader, optimizer, class_weight)\n","    \n","    mean_train_loss = (train09_loss + train13_loss + train19_loss) / 3\n","\n","    scheduler.step(mean_train_loss)\n","    if mean_train_loss < mean_best:\n","        model.save(\"models/\" + fname + \".pmt\")\n","        hist.save(\"models/\" + fname + \".hist\")\n","        mean_best = mean_train_loss\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","\n","    if epoch > 0:\n","        hist.log(epoch, train_loss=round(mean_train_loss, 5),\n","                        best=round(mean_best, 5),\n","                \n","                        tr09_pos=round(train09_acc, 5),\n","                        tr13_pos=round(train13_acc, 5),\n","                        tr19_pos=round(train19_acc, 5),\n","                \n","                        tr09_disp=round(train09_dloss, 5),\n","                        tr13_disp=round(train13_dloss, 5),\n","                        tr19_disp=round(train19_dloss, 5),\n","                \n","                        tr09_wacc=round(train09_wacc, 5),\n","                        tr13_wacc=round(train13_wacc, 5),\n","                        tr19_wacc=round(train19_wacc, 5)\n","                )\n","        with canvas:\n","            canvas.draw_plot([hist[\"train_loss\"],\n","                              hist[\"best\"]])\n","            canvas.draw_plot([hist[\"tr09_pos\"],\n","                              hist[\"tr13_pos\"],\n","                              hist[\"tr19_pos\"]])\n","            canvas.draw_plot([hist[\"tr09_wacc\"],\n","                              hist[\"tr13_wacc\"],\n","                              hist[\"tr19_wacc\"]])\n","            canvas.draw_plot([hist[\"tr09_disp\"],\n","                              hist[\"tr13_disp\"],\n","                              hist[\"tr19_disp\"]])\n","    epoch += 1\n","\n","hist.summary()\n","hist.save(\"models/\" + fname + \".hist\")\n","model.load(\"models/\" + fname + \".pmt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXMUmrelt5bb"},"source":["2x20: 0.01679 - 5.36\n","\n","3x16: 0.01124 - 4.94\n","\n","3x12: 0.01930 - 4.32\n","\n","4x12: 0.00954 - 4.45"]},{"cell_type":"markdown","metadata":{"id":"DDJY_VuwkRw2"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"RcljOoVkdLrD"},"source":["ds = BoardPositionDataset(['reddit.json', 'facebook_fixed.json'], 19, augment=False, create_wrong=False, only_labelled=False)\r\n","\r\n","model = Interpreter()\r\n","fname ='interpreter_32'\r\n","model.load(\"models/\" + fname + \".pmt\")\r\n","model = model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOXRMTTQJowX"},"source":["chunk = 2\n","nplots = 1\n","plt.figure(figsize=(40, 40))\n","start = dt.now()\n","for i in range(36*chunk, min(36*(chunk+1), len(ds))):\n","    img, _, _, _ = ds[i]\n","    pred, _, _ = model(img.unsqueeze(0))\n","    pred = pred.detach().cpu()\n","\n","    plt.subplot(6, 6, i+1 - 36*chunk)\n","    pred = pred.round().squeeze()\n","    plt.imshow(\n","        img.squeeze().cpu().permute(1, 2, 0),\n","        extent=(1,19,1,19))\n","    plt.imshow(\n","        pred.argmax(axis=0),\n","        norm = Normalize(vmin = 0, vmax = 2),\n","        cmap='bwr',\n","        extent=(1,19,1,19),\n","        alpha=0.25)\n","    plt.title(ds.filename[i])\n","print(f'Elapsed: {dt.now() - start}')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqFbCPTLjA3J"},"source":["# Relabelling"]},{"cell_type":"code","metadata":{"id":"HVyp_b1b3rpl"},"source":["with open('to_fix2.txt', 'r') as f:\r\n","    ignore = [line[:-1] + '.jpg' for line in f.readlines()]\r\n","\r\n","model = Interpreter()\r\n","fname ='interpreter_31'\r\n","model.load(\"models/\" + fname + \".pmt\")\r\n","model.eval()\r\n","\r\n","res = []\r\n","for s in [9, 13, 19]:\r\n","    ds = BoardPositionDataset(['reddit.json', 'facebook_fixed.json'], s, augment=False, create_wrong=False, only_labelled=False)\r\n","    for i in range(len(ds)):\r\n","        if ds.filename[i] in ignore: continue\r\n","        img, _, _, _ = ds[i]\r\n","        pred, _, _ = model(img.unsqueeze(0))\r\n","        pred = pred.detach().cpu()\r\n","\r\n","        pred = pred.round().squeeze().argmax(axis=0).transpose(0,1).flipud()\r\n","        res.append(\r\n","            {\r\n","                'filename' : ds.filename[i],\r\n","                'positions' : [el-1 for row in pred.tolist() for el in row],\r\n","                'corners' : ds.orig_corners[i],\r\n","                'size' : ds.size[i]\r\n","            }\r\n","        )\r\n","json.dump(res, open('self_labels.json', 'w'), indent=2, separators=(',', ':'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICyxHwvC_tdI"},"source":["ds = BoardPositionDataset(['self_labels.json'], 19, augment=False, create_wrong=False, only_labelled=False)\r\n","\r\n","plt.figure(figsize=(32, 32))\r\n","for i in range(25):\r\n","    plt.subplot(5, 5, i+1)\r\n","    index = random.randrange(len(ds))\r\n","    img, bpos, _, _ = ds[index]\r\n","    plt.imshow(img.permute(1, 2, 0),\r\n","               extent=(0, 1, 0, 1))\r\n","    plt.imshow(bpos,\r\n","               norm = Normalize(vmin = 0, vmax = 2),\r\n","               cmap='bwr',\r\n","               extent=(0,1,0,1),\r\n","               alpha=0.2)\r\n","    plt.title(ds.filename[index])\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zD8nS7I1H4iw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kInDbyrN9lhr"},"source":["# Tilt distribution"]},{"cell_type":"code","metadata":{"id":"r2Q-CasV9pDq"},"source":["board_extractor = {9 : BoardExtractor(9),\n","                   13: BoardExtractor(13),\n","                   19: BoardExtractor(19)}\n","\n","filename = []\n","tilts = []\n","with open('metadata.json', 'r') as f:\n","    metadata = json.load(f)\n","    for entry in tqdm(metadata):\n","        img = default_loader(\"images/\" + entry['filename'])\n","        if img.size[0] > img.size[1]: \n","            img = img.resize((512,384), resample=Image.BILINEAR)\n","        if img.size[0] < img.size[1]: \n","            img = img.resize((384,512), resample=Image.BILINEAR)\n","        if img.size[0] == img.size[1]: \n","            img = img.resize((384,384), resample=Image.BILINEAR)\n","    \n","        corner = np.array(entry['corners'])\n","        corner[:,0] = corner[:,0] / 100 * img.size[0]\n","        corner[:,1] = corner[:,1] / 100 * img.size[1]\n","\n","        filename.append(entry['filename'])\n","        _, tilt = board_extractor[entry['size']](img, corner)\n","        tilts.append(tilt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuszK4aF_NFQ"},"source":["np.concatenate(tilts, axis=0).max()"],"execution_count":null,"outputs":[]}]}