{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Interpreter.ipynb","provenance":[{"file_id":"1x3tby9J00egepof5936ik6HcUhMoBspu","timestamp":1586548024302}],"private_outputs":true,"collapsed_sections":["0RbKhiS9lJR-","9hpHGqQXgEJe","GewcKSmsAhle","GMQVcc7yAf7E","s25qd-xQj6XD","lCcitpXOp-wj","kInDbyrN9lhr"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ePYKUUMTKyUn","colab_type":"text"},"source":["# Setup enviroment"]},{"cell_type":"code","metadata":{"id":"QqCgg6b9OIgH","colab_type":"code","colab":{}},"source":["!pip3 install hiddenlayer > /dev/null\n","!pip3 install ipdb > /dev/null\n","import json\n","import os\n","import numpy as np\n","import random\n","\n","import torch as tr\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from torchvision.datasets.folder import default_loader\n","from torchvision.transforms import functional as ft\n","from torchvision import transforms\n","from PIL import Image, ImageDraw, ImageFilter\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import balanced_accuracy_score\n","\n","import matplotlib\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import Normalize\n","import hiddenlayer as hl\n","\n","from google.colab import drive\n","\n","from tqdm.notebook import tqdm\n","\n","tr.backends.cudnn.deterministic = False\n","tr.backends.cudnn.benchmark = True\n","tr.manual_seed(42)\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gubZcgYnOnMF","colab_type":"code","colab":{}},"source":["drive.mount('./drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huS-FFXzOppO","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/drive/My Drive/Workspace/pic2sgf/train\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RbKhiS9lJR-","colab_type":"text"},"source":["# Board extractor"]},{"cell_type":"code","metadata":{"id":"8UJcj0RMlLpV","colab_type":"code","colab":{}},"source":["class BoardExtractor():\n","    def __init__(self, board_size):\n","        self.size = board_size * 16\n","        B = np.array([[8, 8, 1],\n","                      [self.size-8, 8, 1],\n","                      [self.size-8, self.size-8, 1],\n","                      [8, self.size-8, 1]]).T\n","        self.T2 = np.linalg.inv(B[:, 0:3] * np.linalg.solve(B[:, 0:3], B[:, 3]))\n","\n","    def __call__(self, img, vertexs):\n","        A = np.concatenate([vertexs.T, np.array([[1.0, 1.0, 1.0, 1.0]])], axis=0)\n","        T1 = A[:, 0:3] * np.linalg.solve(A[:, 0:3], A[:, 3])\n","        T = np.matmul(T1, self.T2)\n","        T /= T[2,2]\n","        board = img.transform((self.size, self.size),\n","                              method=Image.PERSPECTIVE,\n","                              data = T.reshape(-1),\n","                              resample=Image.BILINEAR)\n","        return board.transpose(Image.ROTATE_180), T[2,0:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hpHGqQXgEJe","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"GewcKSmsAhle","colab_type":"text"},"source":["## Class"]},{"cell_type":"code","metadata":{"id":"ezH-NmKKIC07","colab_type":"code","colab":{}},"source":["class BoardPositionDataset(Dataset):\n","    def __init__(self, metadata_file, size):\n","        super(BoardPositionDataset, self).__init__()\n","        self.board_extractor = BoardExtractor(size)\n","        with open(metadata_file, 'r') as f:\n","            metadata = json.load(f)\n","        self.augment_images = True\n","        self.displacement_scale = 4\n","        self.images = []\n","        self.position = []\n","        self.corners = []\n","        self.filename = []\n","        self.upsampler = nn.Upsample(size, mode='bilinear', align_corners=True)\n","        for entry in tqdm(metadata):\n","            if entry['size'] != size: continue\n","\n","            img = self.load_image(\"images/\" + entry['filename'])\n","            corner = np.array(entry['corners'])\n","            corner[:,0] = corner[:,0] / 100 * img.size[0]\n","            corner[:,1] = corner[:,1] / 100 * img.size[1]\n","\n","            self.filename.append(entry['filename'])\n","            self.corners.append(corner)\n","            self.images.append(img)\n","            self.position.append(tr.LongTensor(entry['positions']).\\\n","                                 reshape(size, size).permute(1,0).flip(1) + 1)\n","\n","    def load_image(self, filename):\n","        img = default_loader(filename)\n","        if img.size[0] > img.size[1]: \n","            img = img.resize((512,384), resample=Image.BILINEAR)\n","        if img.size[0] < img.size[1]: \n","            img = img.resize((384,512), resample=Image.BILINEAR)\n","        if img.size[0] == img.size[1]: \n","            img = img.resize((384,384), resample=Image.BILINEAR)\n","        return img\n","\n","\n","    def augment(self, img, pos, dis):\n","        if self.augment_images:\n","            img = ft.adjust_brightness(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","            img = ft.adjust_contrast(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","            img = ft.adjust_gamma(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","            img = ft.adjust_saturation(img, np.clip(np.random.normal(loc=1, scale=0.3), 0.3, 1.8))\n","\n","            if np.random.binomial(1, 0.2): \n","                img = img.filter(ImageFilter.GaussianBlur(radius = np.clip(np.random.normal(loc=0.5, scale=0.25), 0, 1)))\n","\n","            if np.random.binomial(1, 0.5): \n","                img, pos, dis = ft.hflip(img), pos.flip(1), dis.flip(2)\n","                dis[0,:,:] *= -1\n","            # if np.random.binomial(1, 0.5): \n","            #     img, pos, dis = ft.vflip(img), pos.flip(0), dis.flip(1)\n","            #     dis[1,:,:] *= -1\n","        return img, pos, dis\n","\n","    def __len__(self):\n","      return len(self.images)\n","\n","    def __getitem__(self, i):\n","        corner = np.copy(self.corners[i])\n","        wrong = 0\n","        displacement = np.random.normal(loc=0, scale=self.displacement_scale, size=(4,2))\n","        if self.augment_images:\n","            corner += displacement\n","        # if self.create_wrong and np.random.binomial(1, 0.1):\n","        #     wrong = True\n","        #     idx = np.random.randint(0, 4)\n","        #     corner[idx] += np.random.randint(12, 100, size=2) * (2*np.random.binomial([0,1], 0.5)-1)\n","        image, _ = self.board_extractor(self.images[i], corner)\n","\n","        displacement = tr.stack([tr.Tensor(displacement[[2,3,1,0], 0].reshape(2, 2)),\n","                                 tr.Tensor(displacement[[2,3,1,0], 1].reshape(2, 2))], dim=0)\n","        displacement = self.upsampler(displacement.unsqueeze(0)).squeeze()\n","\n","        position = self.position[i].clone()\n","        image, position, displacement = self.augment(image, position, displacement)\n","        return ft.to_tensor(image), position, displacement"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G63CrCiOC1DM","colab_type":"code","colab":{}},"source":["dataset09 = BoardPositionDataset('metadata.json', 9)\n","dataset13 = BoardPositionDataset('metadata.json', 13)\n","dataset19 = BoardPositionDataset('metadata.json', 19)\n","\n","train09, test09 = random_split(dataset09, (len(dataset09)-16, 16))\n","train13, test13 = random_split(dataset13, (len(dataset13)-4, 4))\n","train19, test19 = random_split(dataset19, (len(dataset19)-8, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMQVcc7yAf7E","colab_type":"text"},"source":["## Plots"]},{"cell_type":"code","metadata":{"id":"ooW6MN0n35dm","colab_type":"code","colab":{}},"source":["dataset = dataset09\n","\n","plt.figure(figsize=(16, 16))\n","for i in range(9):\n","    plt.subplot(3, 3, i+1)\n","    index = random.randrange(len(dataset))\n","    img, _, disp = dataset[index]\n","    plt.imshow(img.permute(1, 2, 0),\n","               extent=(0,1,0,1))\n","    plt.imshow(disp[0,:,:],\n","               cmap='seismic',\n","               extent=(0,1,0,1),\n","               norm = Normalize(vmin = -10, vmax = 10),\n","               alpha=0.4)\n","    plt.title(dataset.filename[index])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dAZuGb92Iuk","colab_type":"code","colab":{}},"source":["dataset = dataset13\n","\n","plt.figure(figsize=(16, 16))\n","for i in range(9):\n","    plt.subplot(3, 3, i+1)\n","    index = random.randrange(len(dataset))\n","    img, bpos, _ = dataset[index]\n","    plt.imshow(img.permute(1, 2, 0),\n","               extent=(0, 1, 0, 1))\n","    plt.imshow(bpos,\n","               norm = Normalize(vmin = 0, vmax = 2),\n","               cmap='bwr',\n","               extent=(0,1,0,1),\n","               alpha=0.2)\n","    plt.title(dataset.filename[index])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s25qd-xQj6XD","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"0GJmBtBcj5jO","colab_type":"code","colab":{}},"source":["class iblock(nn.Module):\n","    def __init__(self, dims):\n","        super(iblock, self).__init__()\n","        self.conv_path = nn.Sequential(nn.BatchNorm2d(dims), nn.GELU(),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1),\n","                                       nn.GELU(), nn.BatchNorm2d(dims),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1))\n","        \n","    def forward(self, x):\n","        return x + self.conv_path(x)\n","\n","\n","class GlobalFeatures(nn.Module):\n","    def __init__(self, in_dim, dim):\n","        super(GlobalFeatures, self).__init__()\n","        self.input = nn.Sequential(\n","            nn.Conv2d(in_dim, dim, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(dim),\n","            nn.Conv2d(dim, dim, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(dim),\n","            nn.AdaptiveAvgPool2d(1)\n","            )\n","        self.mixer = nn.Sequential(\n","            nn.Conv2d(in_dim + dim, in_dim, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(in_dim)\n","            )\n","\n","    def forward(self, x):\n","        gf = self.input(x)\n","        gf = nn.functional.interpolate(gf, size=(x.shape[2], x.shape[3]))\n","        x = tr.cat([x, gf], dim=1)\n","        return self.mixer(x)\n","\n","\n","def Pooling(in_dim, out_dim):\n","    return nn.Sequential(nn.BatchNorm2d(in_dim), nn.GELU(),\n","                         nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),\n","                         nn.MaxPool2d(2))\n","\n","\n","class Interpreter(nn.Module):\n","    def __init__(self):\n","        super(Interpreter, self).__init__()\n","        self.conv_blocks = nn.Sequential(\n","            nn.Conv2d(3, 12, kernel_size=2, stride=2),\n","            iblock(12), iblock(12),\n","            Pooling(12, 24),\n","            iblock(24), iblock(24),\n","            Pooling(24, 48),\n","            iblock(48), iblock(48),\n","            Pooling(48, 96),\n","            iblock(96), iblock(96),\n","            GlobalFeatures(96, 48),\n","            nn.GELU(), nn.BatchNorm2d(96)\n","            )\n","        self.displacement = nn.Conv2d(96, 2, kernel_size=1)\n","        self.position = nn.Sequential(\n","            nn.Conv2d(96, 48, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(48),\n","            nn.Conv2d(48, 24, kernel_size=1),\n","            nn.GELU(), nn.BatchNorm2d(24),\n","            nn.Conv2d(24, 3, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_blocks(x)\n","        position = self.position(x)\n","        displacement = self.displacement(x)\n","        return position, displacement\n","      \n","    def load(self, fname):\n","        self.load_state_dict(tr.load(fname, map_location=lambda storage, loc: storage))\n","\n","    def save(self, fname):\n","        tr.save(self.state_dict(), fname)\n","\n","print(sum(p.numel() for p in Interpreter().parameters()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYd6vLA45fzB","colab_type":"text"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"lCcitpXOp-wj","colab_type":"text"},"source":["## Auxiliary"]},{"cell_type":"code","metadata":{"id":"DXwJunSVfs4V","colab_type":"code","colab":{}},"source":["nc = [0.0,0.0,0.0]\n","for ds in [dataset09, dataset13, dataset19]:\n","    for _, pos, _ in ds:\n","        for i in range(3):\n","            nc[i] += (pos==i).sum()\n","tot = sum(nc)\n","class_weight = tr.Tensor([tot / nc[i] for i in range(3)]).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqQyKK9ugkOG","colab_type":"code","colab":{}},"source":["def acc(pos, pred):\n","    pos = pos.cpu().detach().numpy()\n","    pred= pred.cpu().detach().numpy().argmax(axis=1)\n","    res = balanced_accuracy_score(pos.reshape(-1), pred.reshape(-1))\n","    return res\n","\n","def fit(model, data_loader, optimizer, class_weight):\n","    gtrue = []\n","    preds = []\n","    total_loss = 0\n","    total_disp_loss = 0\n","    for img, pos, disp in data_loader:\n","        img, pos, disp = img.cuda(), pos.cuda(), disp.cuda()\n","        ppos, pdisp = model(img)\n","\n","        optimizer.zero_grad()\n","        pos_loss  = nn.functional.cross_entropy(ppos, pos, weight=class_weight)\n","        disp_loss = nn.functional.mse_loss(pdisp, disp)\n","        loss = pos_loss + 0.1*disp_loss\n","        loss.backward()\n","        optimizer.step()\n","\n","        gtrue.append( pos)\n","        preds.append(ppos)\n","\n","        total_loss += loss.data.item() / len(data_loader)\n","        total_disp_loss += disp_loss.mean().data.item() / len(data_loader)\n","    pos_acc = acc(tr.cat(gtrue), tr.cat(preds))\n","    return total_loss, pos_acc, total_disp_loss\n","\n","def evaluate_model(model, data_loader, class_weight):\n","    gtrue = []\n","    preds = []\n","    total_loss = 0\n","    total_disp_loss = 0\n","    for img, pos, disp in data_loader:\n","        img, pos, disp = img.cuda(), pos.cuda(), disp.cuda()\n","        ppos, pdisp = model(img)\n","\n","        pos_loss = nn.functional.cross_entropy(ppos, pos, weight=class_weight)\n","        disp_loss = nn.functional.mse_loss(pdisp, disp)\n","        total_loss += (pos_loss + 0.1*disp_loss).item() / len(data_loader)\n","        total_disp_loss += disp_loss.mean().item()\n","\n","        gtrue.append( pos)\n","        preds.append(ppos)\n","    test_acc = acc(tr.cat(gtrue), tr.cat(preds))\n","    return test_acc, total_loss, total_disp_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fDR6KOdERUf5","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"jGpuoL42l4jk","colab_type":"code","colab":{}},"source":["model = Interpreter().cuda()\n","tr.manual_seed(42)\n","np.random.seed(42)\n","\n","fname ='interpreter_35'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU1yOy9df1oo","colab_type":"code","colab":{}},"source":["optimizer = tr.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOhFPqxf5eqs","colab_type":"code","colab":{}},"source":["hist = hl.History()\n","canvas = hl.Canvas()\n","\n","if os.path.isfile(\"models/\" + fname + \".pmt\"):\n","    model.load(\"models/\" + fname + \".pmt\")\n","    hist.load(\"models/\" + fname + \".hist\")\n","    epoch = len(hist['mean_best'].data) + 1\n","    mean_best = min(hist['mean_best'].data)\n","else:\n","    epoch = 0\n","    mean_best = float('inf')\n","\n","dataset09.augment_images = True\n","dataset13.augment_images = True\n","dataset19.augment_images = True\n","\n","train09_loader = DataLoader(train09, batch_size=8, shuffle=True)\n","train13_loader = DataLoader(train13, batch_size=8, shuffle=True)\n","train19_loader = DataLoader(train19, batch_size=8, shuffle=True)\n","test09_loader = DataLoader(test09, batch_size=8, shuffle=True)\n","test13_loader = DataLoader(test13, batch_size=8, shuffle=True)\n","test19_loader = DataLoader(test19, batch_size=8, shuffle=True)\n","      \n","epochs_without_improvement = 0  \n","while epochs_without_improvement < 200:\n","    model.train()\n","    train09_loss, train09_acc, train09_dloss = fit(model, train09_loader, optimizer, class_weight)\n","    train19_loss, train19_acc, train19_dloss = fit(model, train19_loader, optimizer, class_weight)\n","    train13_loss, train13_acc, train13_dloss = fit(model, train13_loader, optimizer, class_weight)\n","\n","    model.eval()\n","    test09_acc, test09_loss, test09_dloss = evaluate_model(model, test09_loader, class_weight)\n","    test19_acc, test19_loss, test19_dloss = evaluate_model(model, test19_loader, class_weight)\n","    test13_acc, test13_loss, test13_dloss = evaluate_model(model, test13_loader, class_weight)\n","    \n","    mean_loss = (test09_loss + test13_loss + test19_loss) / 3\n","\n","    scheduler.step(mean_loss)\n","    if mean_loss < mean_best:\n","        model.save(\"models/\" + fname + \".pmt\")\n","        hist.save(\"models/\" + fname + \".hist\")\n","        mean_best = mean_loss\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","\n","    hist.log(epoch, mean_loss=mean_loss,\n","                    mean_best=mean_best,\n","                    train09_acc=train09_acc,\n","                    test09_acc =test09_acc,\n","                    train13_acc=train13_acc,\n","                    test13_acc =test13_acc,\n","                    train19_acc=train19_acc,\n","                    test19_acc =test19_acc,\n","                    test09_dloss=test09_dloss,\n","                    test13_dloss=test13_dloss,\n","                    test19_dloss=test19_dloss\n","             )\n","\n","    if epoch > 0:\n","        with canvas:\n","            canvas.draw_plot([hist[\"mean_loss\"],\n","                              hist[\"mean_best\"]])\n","            canvas.draw_plot([hist[\"train09_acc\"],\n","                              hist[\"train13_acc\"],\n","                              hist[\"train19_acc\"]])\n","            canvas.draw_plot([hist[\"test09_acc\"],\n","                              hist[\"test13_acc\"],\n","                              hist[\"test19_acc\"]])\n","            canvas.draw_plot([hist[\"test09_dloss\"],\n","                              hist[\"test13_dloss\"],\n","                              hist[\"test19_dloss\"]])\n","    epoch += 1\n","\n","hist.summary()\n","hist.save(\"models/\" + fname + \".hist\")\n","model.load(\"models/\" + fname + \".pmt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3UwzGMHKKfj","colab_type":"text"},"source":["12 - 2 - 495233 = 0.08741\n","\n","10 - 2 - 344455 = 0.1020\n","\n","10 - 3 - 498355 = 0.0987\n","\n","12 - 1 - 273833 = 0.1222\n","\n","12 - 3 - 716633 = 0.0901\n","\n","12 - 2 - 525473 - MP = 0.07692"]},{"cell_type":"markdown","metadata":{"id":"DDJY_VuwkRw2","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"jOXRMTTQJowX","colab_type":"code","colab":{}},"source":["model.load(\"models/\" + fname + \".pmt\")\n","model.eval()\n","\n","ds = dataset19\n","# ds.augment_images = False\n","\n","nplots = 1\n","plt.figure(figsize=(20, 20))\n","for i, sample in enumerate(ds):\n","    img, pos = sample[0].unsqueeze(0).cuda(), sample[1].unsqueeze(0)\n","    pred, _ = model(img)\n","    pred = pred.detach().cpu()\n","\n","    if acc(pos, pred) < 1.0:\n","        if nplots > 16: continue\n","        plt.subplot(4, 4, nplots)\n","        nplots += 1\n","        pred = pred.round().squeeze()\n","        plt.imshow(img.squeeze().cpu().permute(1, 2, 0),\n","                    extent=(0,1,0,1))\n","        plt.imshow(pred.argmax(axis=0),\n","                   norm = Normalize(vmin = 0, vmax = 2),\n","                   cmap='bwr',\n","                   extent=(0,1,0,1),\n","                   alpha=0.25)\n","        plt.title(ds.filename[i])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kInDbyrN9lhr","colab_type":"text"},"source":["# Tilt distribution"]},{"cell_type":"code","metadata":{"id":"r2Q-CasV9pDq","colab_type":"code","colab":{}},"source":["board_extractor = {9 : BoardExtractor(9),\n","                   13: BoardExtractor(13),\n","                   19: BoardExtractor(19)}\n","\n","filename = []\n","tilts = []\n","with open('metadata.json', 'r') as f:\n","    metadata = json.load(f)\n","    for entry in tqdm(metadata):\n","        img = default_loader(\"images/\" + entry['filename'])\n","        if img.size[0] > img.size[1]: \n","            img = img.resize((512,384), resample=Image.BILINEAR)\n","        if img.size[0] < img.size[1]: \n","            img = img.resize((384,512), resample=Image.BILINEAR)\n","        if img.size[0] == img.size[1]: \n","            img = img.resize((384,384), resample=Image.BILINEAR)\n","    \n","        corner = np.array(entry['corners'])\n","        corner[:,0] = corner[:,0] / 100 * img.size[0]\n","        corner[:,1] = corner[:,1] / 100 * img.size[1]\n","\n","        filename.append(entry['filename'])\n","        _, tilt = board_extractor[entry['size']](img, corner)\n","        tilts.append(tilt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuszK4aF_NFQ","colab_type":"code","colab":{}},"source":["np.concatenate(tilts, axis=0).max()"],"execution_count":null,"outputs":[]}]}