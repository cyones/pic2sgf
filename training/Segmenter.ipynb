{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Segmenter.ipynb","private_outputs":true,"provenance":[{"file_id":"1x3tby9J00egepof5936ik6HcUhMoBspu","timestamp":1586199405623}],"collapsed_sections":["ePYKUUMTKyUn","mQNSybgb0Mhs","8BD_OYaY0Ips","4EuDPfsaDZWI"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ePYKUUMTKyUn"},"source":["# Setup enviroment"]},{"cell_type":"code","metadata":{"id":"QqCgg6b9OIgH"},"source":["!pip3 install hiddenlayer > /dev/null\n","!pip3 install ipdb > /dev/null\n","import json\n","import os\n","import numpy as np\n","import random\n","import time\n","\n","import torch as tr\n","import torch.nn as nn\n","from torch.nn import BCELoss\n","from scipy import stats\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from torchvision.datasets.folder import default_loader\n","from torchvision.transforms import functional as ft\n","from torchvision import transforms\n","from PIL import Image, ImageDraw, ImageFilter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import Normalize\n","import hiddenlayer as hl\n","\n","from google.colab import drive\n","\n","from tqdm.notebook import tqdm\n","\n","from scipy import ndimage\n","\n","tr.backends.cudnn.deterministic = False\n","tr.backends.cudnn.benchmark = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gubZcgYnOnMF"},"source":["drive.mount('./drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huS-FFXzOppO"},"source":["os.chdir(\"/content/drive/My Drive/Workspace/pic2sgf/training\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bS_oK539A3p"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"mQNSybgb0Mhs"},"source":["## Metadata fixing"]},{"cell_type":"code","metadata":{"id":"TWOrFY680Q-P"},"source":["with open('facebook.json', 'r') as f:\n","    metadata = json.load(f)\n","\n","for i in tqdm(range(len(metadata))):\n","    filename = metadata[i]['filename']\n","    img = default_loader(\"images/\" + filename)\n","    if img.size[0] < img.size[1]:\n","        ccoords = [[row[1], 100-row[0]] for row in metadata[i]['corners']]\n","        metadata[i]['corners'] = ccoords\n","\n","with open('facebook_fixed.json', 'w') as f:\n","    json.dump(metadata, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8BD_OYaY0Ips"},"source":["## Dataset loading"]},{"cell_type":"code","metadata":{"id":"gqMQDxxOO9Ou"},"source":["class VertexSegmenterDataset(Dataset):\n","    def __init__(self, metadata_files):\n","        super(Dataset, self).__init__()\n","        self.filename = []\n","        self.corner_coords =  []\n","        self.label = []\n","        self.image = []\n","\n","        for file in metadata_files:\n","            with open(file, 'r') as f:\n","                metadata = json.load(f)\n","            self.filename.extend([entry['filename'] for entry in metadata])\n","            self.corner_coords.extend([entry['corners'] for entry in metadata])\n","        \n","        self.image = [self.load_image(\"images/\" + filename) for filename in tqdm(self.filename)]\n","        self.label = [self.gen_label(corner) for corner in self.corner_coords]\n","\n","    def load_image(self, filename):\n","        img = default_loader(filename)\n","        if img.size[0] > img.size[1]:\n","            img = img.resize((512, 384), resample=Image.BILINEAR)\n","        else:\n","            img = img.resize((384, 512), resample=Image.BILINEAR).transpose(Image.ROTATE_90)\n","        return img\n","\n","    def gen_label(self, corners):\n","        w, h = int(512 / 2), int(384 / 2)\n","        im = Image.new('RGB', (w, h))\n","        draw = ImageDraw.Draw(im)\n","        coords = [(int(c[0] * w / 100), int(c[1] * h / 100)) for c in corners]\n","        draw.polygon(coords, fill=(255, 0, 0), outline=(255, 255, 0))\n","        for c in coords:\n","            draw.ellipse([c[0]-1, c[1]-1, c[0]+1, c[1]+1], fill=(255, 255, 255))\n","        im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n","        return im\n","    \n","    def augment(self, img, lbl):\n","        img = ft.adjust_brightness(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","        img = ft.adjust_contrast(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","        img = ft.adjust_gamma(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","        img = ft.adjust_saturation(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","\n","        if np.random.binomial(1, 0.1): \n","            img = img.filter(ImageFilter.GaussianBlur(radius = np.clip(np.random.normal(loc=2, scale=1), 0, 2)))\n","\n","        if np.random.binomial(1, 0.5):\n","            img = ft.vflip(img)\n","            lbl = ft.vflip(lbl)\n","\n","        if np.random.binomial(1, 0.5):\n","            img = ft.hflip(img)\n","            lbl = ft.hflip(lbl)\n","\n","        angle = np.random.random()*180 - 90\n","        img_size, lbl_size = img.size, lbl.size\n","        fill_color = (np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255))\n","        img = ft.rotate(img, angle, resample=Image.NEAREST, expand=True, fill=fill_color)\n","        lbl = ft.rotate(lbl, angle, resample=Image.BICUBIC, expand=True)\n","        img = img.resize(img_size, resample=Image.BILINEAR)\n","        lbl = lbl.resize(lbl_size, resample=Image.BICUBIC)\n","        \n","        img, lbl = ft.to_tensor(img), ft.to_tensor(lbl)\n","        mv = lbl.max(dim=1)[0].max(dim=1)[0]\n","        lbl = lbl / mv.unsqueeze(1).unsqueeze(1)\n","        return img, lbl\n","\n","    def __len__(self):\n","      return len(self.filename)\n","\n","    def __getitem__(self, i):\n","        image = self.image[i]\n","        label = self.label[i]\n","        return self.augment(image, label)\n","\n","dataset = VertexSegmenterDataset(['facebook_fixed.json', 'reddit.json', 'metadata.json'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOB1334f5XjX"},"source":["plt.figure(figsize=(20,40))\n","nplot = 1\n","for i in range(40):\n","    plt.subplot(10, 4, nplot)\n","    nplot += 1\n","    index = random.randrange(len(dataset))\n","    img, lbl = dataset[index]\n","    plt.title(dataset.filename[index])\n","    plt.imshow(img.permute(1,2,0),\n","               extent=(0, 1, 0, 1))\n","    plt.imshow(lbl.permute(1,2,0),\n","               extent=(0, 1, 0, 1),\n","               alpha=0.25)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4EuDPfsaDZWI"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"TNmo8reX9JvD"},"source":["class iblock(nn.Module):\n","    def __init__(self, dims):\n","        super(iblock, self).__init__()\n","        self.conv_path = nn.Sequential(nn.BatchNorm2d(dims), nn.GELU(),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1),\n","                                       nn.GELU(), nn.BatchNorm2d(dims),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1))\n","        \n","    def forward(self, x):\n","        return x + self.conv_path(x)\n","\n","\n","def Pooling(in_dim, out_dim):\n","    return nn.Sequential(nn.BatchNorm2d(in_dim), nn.GELU(),\n","                         nn.Conv2d(in_dim, out_dim, kernel_size=2, stride=2))\n","\n","\n","class Segmenter(nn.Module):\n","    def __init__(self):\n","        super(Segmenter, self).__init__()\n","        self.downscale = nn.ModuleList([Pooling(10, 20), Pooling(20, 40), Pooling(40, 80), Pooling(80, 80)])\n","        self.upscale = nn.ModuleList([nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)])\n","        \n","        self.pre_cnn = nn.Sequential(\n","            nn.Conv2d(3, 12, kernel_size=2, stride=2),\n","            nn.BatchNorm2d(12), nn.GELU(),\n","            nn.Conv2d(12, 10, kernel_size=1),\n","            )\n","\n","        self.in_cnn = nn.ModuleList([nn.Sequential(iblock(10), iblock(10), iblock(10)),\n","                                     nn.Sequential(iblock(20), iblock(20), iblock(20)),\n","                                     nn.Sequential(iblock(40), iblock(40), iblock(40)),\n","                                     nn.Sequential(iblock(80), iblock(80), iblock(80))\n","                                     ])\n","\n","        self.bottom = nn.Sequential(iblock(80), iblock(80),\n","                                    nn.BatchNorm2d(80), nn.GELU(),\n","                                    nn.Conv2d(80, 80, kernel_size=1))\n","\n","        self.out_cnn = nn.ModuleList([nn.Sequential(iblock(80), iblock(80),\n","                                                    nn.GELU(), nn.BatchNorm2d(80),\n","                                                    nn.Conv2d(80, 40, kernel_size=1)),\n","                                      nn.Sequential(iblock(40), iblock(40),\n","                                                    nn.GELU(), nn.BatchNorm2d(40),\n","                                                    nn.Conv2d(40, 20, kernel_size=1)),\n","                                      nn.Sequential(iblock(20), iblock(20),\n","                                                    nn.GELU(), nn.BatchNorm2d(20),\n","                                                    nn.Conv2d(20, 10, kernel_size=1)),\n","                                      nn.Sequential(iblock(10), iblock(10),\n","                                                    nn.GELU(), nn.BatchNorm2d(10))\n","        ])\n","            \n","        self.last_cnn = nn.Sequential(nn.Conv2d(10, 3, kernel_size=1), nn.Sigmoid())\n","\n","    def forward(self, x):\n","        x = self.pre_cnn(x)\n","        mid = []\n","        for i in range(len(self.in_cnn)):\n","            x = self.in_cnn[i](x)\n","            mid.append(x)\n","            x = self.downscale[i](x)\n","        \n","        x = self.bottom(x)\n","\n","        for i in range(len(self.out_cnn)):\n","            x = self.out_cnn[i]( self.upscale[i](x) + mid.pop() )\n","        x = self.last_cnn(x)\n","        return x\n","\n","    def load(self, fname):\n","        self.load_state_dict(tr.load(fname, map_location=lambda storage, loc: storage))\n","\n","    def save(self, fname):\n","        tr.save(self.state_dict(), fname)\n","\n","npar = sum(p.numel() for p in Segmenter().parameters())\n","print(f\"{npar} parameters\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvhqWHT1Czgf"},"source":["class DiceLoss(tr.nn.Module):\n","    def __init__(self, layer_weights):\n","        super(DiceLoss, self).__init__()\n","        self.layer_weights = layer_weights\n","        self.smooth = 1e-6\n","\n","    def forward(self, pred, target):\n","        intersection = (pred * target).sum(2).sum(2)\n","        sum_A = pred.sum(2).sum(2)\n","        sum_B = target.sum(2).sum(2)\n","        loss = 1 - (2 * intersection + self.smooth) / (sum_A + sum_B + self.smooth)\n","        loss = 100 * loss.mean(0)\n","        return loss, loss * self.layer_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPyi7uv0eoMe"},"source":["class IoTLoss(tr.nn.Module):\n","    def __init__(self, layer_weights):\n","        super(IoTLoss, self).__init__()\n","        self.layer_weights = layer_weights\n","        self.smooth = 1e-6\n","\n","    def forward(self, pred, target):\n","        intersection = (pred * target).sum(2).sum(2)\n","        union = tr.max(pred, target).sum(2).sum(2)\n","        loss = 1 - (intersection + self.smooth) / (union + self.smooth)\n","        loss = 100 * loss.mean(0)\n","        return loss, loss * self.layer_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0P62IQUtFwE"},"source":["class WBCELoss(tr.nn.Module):\n","    def __init__(self, layer_weights):\n","        super(WBCELoss, self).__init__()\n","        self.layer_weights = layer_weights\n","        self.bce = BCELoss(reduction='none')\n","\n","    def forward(self, pred, target):\n","        loss = 1000 * self.bce(pred, target).mean(2).mean(2).mean(0)\n","        return loss, loss * self.layer_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQFtgrVqDbYV"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"HRhbHtGkzAgH"},"source":["## Base training"]},{"cell_type":"code","metadata":{"id":"7tUwnJ4Xjrl9"},"source":["model = Segmenter().cuda()\n","\n","fname ='segmenter_2'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsKBze-cDdg1"},"source":["hist = hl.History()\n","canvas = hl.Canvas()\n","\n","epoch = 0\n","best_corner = float('inf')\n","best_total = float('inf')\n","loss_func = None\n","\n","if os.path.isfile(\"models/\" + fname + \".pmt\"):\n","    model.load(\"models/\" + fname + \".pmt\")\n","    hist.load(\"models/\" + fname + \".hist\")\n","    epoch = len(hist['best_total'].data) + 1\n","    best_total = min(hist['best_total'].data)\n","    best_corner = min(hist['best_corner'].data)\n","    train_total=best_total\n","    train_corner=best_corner\n","\n","\n","\n","# loss_func = WBCELoss(tr.Tensor([1.0, 0.5, 0.1]).cuda())\n","# optimizer = tr.optim.Adam(model.parameters(), lr=0.02, weight_decay=1e-5)\n","# scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-9)\n","# train_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n","\n","# loss_func = IoTLoss(tr.Tensor([1.0, 1.0, 1.0]).cuda())\n","# optimizer = tr.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n","# scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-9)\n","# train_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n","\n","loss_func = IoTLoss(tr.Tensor([0.2, 0.01, 1.0]).cuda())\n","optimizer = tr.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9)\n","scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=20, min_lr=1e-9)\n","train_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n","\n","epochs_without_improvement = 0\n","while epochs_without_improvement < 300:\n","    model.train()\n","    train_total *= 0.5\n","    train_corner *= 0.5\n","    for img, lbl in train_loader:\n","        img, lbl = img.cuda(), lbl.cuda()\n","        pred = model(img)\n","        \n","        optimizer.zero_grad()\n","        loss, wloss = loss_func(pred, lbl)\n","\n","        wloss.mean().backward()\n","        optimizer.step()\n","        \n","        train_total += 0.5 * loss.mean().data.item() / len(train_loader)\n","        train_corner += 0.5 * loss[2].data.item() / len(train_loader)\n","    \n","    scheduler.step(train_corner)\n","    if train_corner < best_corner:\n","        if epoch > 50:\n","            model.save(\"models/\" + fname + \".pmt\")\n","            hist.save(\"models/\" + fname + \".hist\")\n","        best_corner = train_corner\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","    best_total = min(train_total, best_total)\n","        \n","    if epoch > 0:\n","        hist.log(epoch, train_total = train_total,\n","                        best_total = best_total,\n","                        train_corner= train_corner,\n","                        best_corner= best_corner)\n","\n","        with canvas:\n","            canvas.draw_plot([hist[\"train_total\"],\n","                              hist[\"best_total\"]])\n","            canvas.draw_plot([hist[\"train_corner\"],\n","                              hist[\"best_corner\"]])\n","    epoch += 1\n","\n","hist.summary()\n","hist.save(\"models/\" + fname + \".hist\")\n","model.load(\"models/\" + fname + \".pmt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKOjX6gnRqFX"},"source":["\n","ReLU: 54.1864\n","\n","GELU: 53.6563\n","\n","last_128: 54.2754\n","\n","last_64: 53.7557\n","\n","w10: 52.8409\n","\n","w12: 52.4468"]},{"cell_type":"markdown","metadata":{"id":"QVvtYJEOzDYk"},"source":["## Finetunning"]},{"cell_type":"code","metadata":{"id":"J4T7ov4UJo9w"},"source":["hist = hl.History()\n","canvas = hl.Canvas()\n","\n","model.load(\"models/\" + fname + \".pmt\")\n","epoch = 0\n","best_total = float('inf')\n","best_corner = float('inf')\n","\n","\n","loss_func = IoTLoss(tr.Tensor([0.1, 0.1, 1.0]).cuda())\n","optimizer = tr.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-6)\n","scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-9)\n","train_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n","epochs_without_improvement = 0\n","\n","train_total = 40\n","train_corner = 53\n","while epochs_without_improvement < 100:\n","    model.train()\n","    train_total *= 0.5\n","    train_corner *= 0.5\n","    batch_loss = 0\n","    optimizer.zero_grad()\n","    for img, lbl in train_loader:\n","        img, lbl = img.cuda(), lbl.cuda()\n","        pred = model(img)\n","        \n","        loss, wloss = loss_func(pred, lbl)\n","        wloss = wloss.mean() / len(train_loader)\n","        wloss.backward()\n","\n","        train_total += 0.5 * loss.mean().data.item() / len(train_loader)\n","        train_corner += 0.5 * loss[2].data.item() / len(train_loader)\n","\n","    optimizer.step()\n","\n","    \n","    scheduler.step(train_corner)\n","    if train_corner < best_corner:\n","        if epoch > 10:\n","            model.save(\"models/\" + fname + \"_ft.pmt\")\n","            hist.save(\"models/\" + fname + \"_ft.hist\")\n","        best_corner = train_corner\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","    best_total = min(train_total, best_total)\n","        \n","    if epoch > 0:\n","        hist.log(epoch, train_total = train_total,\n","                        best_total = best_total,\n","                        train_corner= train_corner,\n","                        best_corner= best_corner)\n","\n","        with canvas:\n","            canvas.draw_plot([hist[\"train_total\"],\n","                              hist[\"best_total\"]])\n","            canvas.draw_plot([hist[\"train_corner\"],\n","                              hist[\"best_corner\"]])\n","    epoch += 1\n","\n","hist.summary()\n","hist.save(\"models/\" + fname + \"_ft.hist\")\n","model.load(\"models/\" + fname + \"_ft.pmt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j6_N91h0Rxu5"},"source":["# Test\n"]},{"cell_type":"markdown","metadata":{"id":"JygkUq5G3q-T"},"source":["## Corner detector"]},{"cell_type":"code","metadata":{"id":"PyAFHf10YfcE"},"source":["fname ='segmenter_2'\r\n","params_path = \"models/\" + fname + \".pmt\"\r\n","unet = Segmenter()\r\n","unet.load(params_path)\r\n","unet = unet.cpu()\r\n","unet.save('models/segmenter_3.pmt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfZN4bIkoUjD"},"source":["fname ='segmenter_2'\n","params_path = \"models/\" + fname + \".pmt\"\n","\n","class CornerDetector():\n","    def __init__(self, gpu=False):\n","        self.unet = Segmenter()\n","        self.unet.load(params_path)\n","        self.unet.eval()\n","        if gpu: \n","            self.unet = self.unet.cuda()\n","        self.gpu = gpu\n","\n","    def segment(self, image):\n","        tensor = ft.to_tensor(image).unsqueeze(0)\n","        if self.gpu:\n","            tensor = tensor.cuda()\n","        segmentation = self.unet(tensor)\n","        segmentation = segmentation.detach().cpu().numpy().squeeze()\n","        segmentation[segmentation < 0.1] = 0.0\n","        return segmentation\n","\n","    def detect_corner(self, image, seg):\n","        segmentation = np.copy(seg)\n","        ccomponent, ncomponent = ndimage.label(segmentation[0])\n","        greather_component = stats.mode(ccomponent[ccomponent>0], axis=None)[0]\n","        segmentation = segmentation[2]\n","        segmentation[ccomponent != greather_component] = 0.0\n","        \n","        ccomponent, ncomponent = ndimage.label(segmentation)\n","        if ncomponent < 4: raise Exception(f\"Missing {4 - ncomponent} corners.\")\n","\n","        confidence = np.zeros((4))\n","        vertexs = -np.ones((4, 2))\n","        for i in range(4):\n","            max_probability = segmentation.max()\n","            confidence[i] = max_probability\n","            max_position = np.where(segmentation == max_probability)\n","\n","            mask = ccomponent[max_position[0][0], max_position[1][0]] == ccomponent\n","            p = np.where(mask)\n","            w = segmentation[mask]\n","            w /= w.sum()\n","            vertexs[i] = np.array([(p[0] * w).sum(), (p[1] * w).sum()])\n","            segmentation[mask] = 0.0\n","        vertexs = 2 * vertexs[:,[1,0]]\n","        idxs = self.order_vertexs(vertexs, image.size)\n","        return vertexs[idxs]\n","\n","    def order_vertexs(self, v, img_size):\n","        w, h = img_size\n","        vc = v.copy()\n","        idxs = np.ones(4).astype(int)\n","        idxs[0] = np.linalg.norm(vc, ord=2, axis=1).argmin()\n","        vc[idxs[0]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[1] = np.linalg.norm(vc - np.array([w,0]), ord=2, axis=1).argmin()\n","        vc[idxs[1]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[2] = np.linalg.norm(vc - np.array([w,h]), ord=2, axis=1).argmin()\n","        vc[idxs[2]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[3] = np.linalg.norm(vc - np.array([0,h]), ord=2, axis=1).argmin()\n","        vc[idxs[3]] = np.array([float('inf'), float('inf')])\n","\n","        last_prod = 0\n","        for i in range(len(v)):\n","            prev = v[idxs[(i-1)%4]] - v[idxs[i]]\n","            post = v[idxs[(i+1)%4]] - v[idxs[i]]\n","            cross_prod = np.cross(post, prev)\n","            if cross_prod * last_prod < 0:\n","                idxs[i], idxs[(i+1)%4] = idxs[(i+1)%4].copy(), idxs[i].copy()\n","                i += 1\n","            else:\n","                last_prod = cross_prod\n","        return idxs\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7Am5R4D3tu1"},"source":["## Run tests"]},{"cell_type":"code","metadata":{"id":"GhK68bv4-u8h"},"source":["vertex_detector = CornerDetector(gpu=False)\n","\n","plt.figure(figsize=(20,40))\n","for i in range(8*4):\n","    plt.subplot(8, 4, i+1)\n","    img, _ = dataset[i+350]\n","    img = ft.to_pil_image(img)\n","\n","    segmentation = vertex_detector.segment(img)\n","    try:\n","        pred_vertex = vertex_detector.detect_corner(img, segmentation)\n","    except:\n","        print('Corner missing!')\n","        continue\n","    draw = ImageDraw.Draw(img)\n","    draw.polygon([(pred_vertex[i][0], pred_vertex[i][1]) for i in range(4)])\n","    del draw\n","    plt.imshow(img, extent = (0,1,0,1))\n","    plt.imshow(segmentation[2],\n","                cmap='jet',\n","                norm=Normalize(0, 1),\n","                extent = (0,1,0,1),\n","                alpha=0.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5O6L_ojn214"},"source":["vertex_detector = CornerDetector(gpu=False)\n","\n","nplots = 0\n","dist = []\n","plt.figure(figsize=(20,40))\n","nmc = 0\n","for i in tqdm(range(len(dataset))):\n","    img = dataset.image[i]\n","    true_vertex = np.array(dataset.corner_coords[i])\n","    segmentation = vertex_detector.segment(img)\n","    fail = False\n","    try:\n","        pred_vertex = vertex_detector.detect_corner(img, segmentation)\n","    except:\n","        fail = True\n","        nmc += 1\n","        continue\n","\n","    true_vertex[:,0] *= 512 / 100\n","    true_vertex[:,1] *= 384 / 100\n","    \n","    vo = vertex_detector.order_vertexs(true_vertex, img.size)\n","    true_vertex = true_vertex[vo]\n","\n","    d = np.sqrt(((pred_vertex - true_vertex)**2).sum(1))\n","    dist.append(d)\n","\n","    if ((d > 30).any() or fail) and nplots < 20:\n","        plt.subplot(5, 4, nplots+1)\n","        nplots += 1\n","        draw = ImageDraw.Draw(img)\n","        draw.polygon([(pred_vertex[i][0], pred_vertex[i][1]) for i in range(4)])\n","        del draw\n","        plt.imshow(img, extent = (0,1,0,1))\n","        plt.imshow(segmentation[2],\n","                    cmap='jet',\n","                    norm=Normalize(0, 1),\n","                    extent = (0,1,0,1),\n","                    alpha=0.5)\n","        plt.title(dataset.filename[i])\n","print(f\"Missing corners: {nmc}\")\n","dist = np.concatenate(dist)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkMBYWOAWNt2"},"source":["dist[dist<50].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRo9uoRnX7kh"},"source":["np.histogram(dist)"],"execution_count":null,"outputs":[]}]}