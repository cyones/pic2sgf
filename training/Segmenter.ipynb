{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Segmenter.ipynb","provenance":[{"file_id":"1x3tby9J00egepof5936ik6HcUhMoBspu","timestamp":1586199405623}],"private_outputs":true,"collapsed_sections":["ePYKUUMTKyUn","3bS_oK539A3p"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ePYKUUMTKyUn","colab_type":"text"},"source":["# Setup enviroment"]},{"cell_type":"code","metadata":{"id":"QqCgg6b9OIgH","colab_type":"code","colab":{}},"source":["!pip3 install hiddenlayer > /dev/null\n","!pip3 install ipdb > /dev/null\n","import json\n","import os\n","import numpy as np\n","import random\n","import time\n","\n","import torch as tr\n","import torch.nn as nn\n","from torch.nn import BCELoss\n","from scipy import stats\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from torchvision.datasets.folder import default_loader\n","from torchvision.transforms import functional as ft\n","from torchvision import transforms\n","from PIL import Image, ImageDraw, ImageFilter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import Normalize\n","import hiddenlayer as hl\n","\n","from google.colab import drive\n","\n","from tqdm.notebook import tqdm\n","\n","from scipy import ndimage\n","\n","tr.backends.cudnn.deterministic = False\n","tr.backends.cudnn.benchmark = True\n","tr.manual_seed(42)\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gubZcgYnOnMF","colab_type":"code","colab":{}},"source":["drive.mount('./drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huS-FFXzOppO","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/drive/My Drive/Workspace/pic2sgf/train\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bS_oK539A3p","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"gqMQDxxOO9Ou","colab_type":"code","colab":{}},"source":["class VertexSegmenterDataset(Dataset):\n","    def __init__(self, metadata_file):\n","        super(Dataset, self).__init__()\n","        with open(metadata_file, 'r') as f:\n","            metadata = json.load(f)\n","        self.filename = [entry['filename'] for entry in metadata]\n","        self.corner_coords =  [entry['corners'] for entry in metadata]\n","        self.image = [self.load_image(\"images/\" + filename) for filename in tqdm(self.filename)]\n","        self.label = [self.gen_label(corner) for corner in self.corner_coords]\n","\n","    def load_image(self, filename):\n","        img = default_loader(filename)\n","        if img.size[0] > img.size[1]:\n","            img = img.resize((512, 384), resample=Image.BILINEAR)\n","        else:\n","            img = img.resize((384, 512), resample=Image.BILINEAR).transpose(Image.ROTATE_90)\n","        return img\n","\n","    def gen_label(self, corners):\n","        w, h = int(512 / 2), int(384 / 2)\n","        im = Image.new('RGB', (w, h))\n","        draw = ImageDraw.Draw(im)\n","        coords = [(int(c[0] * w / 100), int(c[1] * h / 100)) for c in corners]\n","        draw.polygon(coords, fill=(255, 0, 0), outline=(255, 255, 0))\n","        for c in coords:\n","            # draw.point([c[0], c[1]], fill=(255, 255, 255))\n","            draw.ellipse([c[0]-1, c[1]-1, c[0]+1, c[1]+1], fill=(255, 255, 255))\n","        im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n","        return im\n","    \n","    def augment(self, img, lbl):\n","        img = ft.adjust_brightness(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","        img = ft.adjust_contrast(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","        img = ft.adjust_gamma(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","        img = ft.adjust_saturation(img, np.clip(np.random.normal(loc=1, scale=0.2), 0, 2))\n","\n","        if np.random.binomial(1, 0.1): \n","            img = img.filter(ImageFilter.GaussianBlur(radius = np.clip(np.random.normal(loc=2, scale=1), 0, 2)))\n","\n","        if np.random.binomial(1, 0.5):\n","            img = ft.vflip(img)\n","            lbl = ft.vflip(lbl)\n","\n","        if np.random.binomial(1, 0.5):\n","            img = ft.hflip(img)\n","            lbl = ft.hflip(lbl)\n","\n","        angle = np.random.random()*180 - 90\n","        img_size, lbl_size = img.size, lbl.size\n","        fill_color = (np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255))\n","        img = ft.rotate(img, angle, resample=Image.NEAREST, expand=True, fill=fill_color)\n","        lbl = ft.rotate(lbl, angle, resample=Image.BICUBIC, expand=True)\n","        img = img.resize(img_size, resample=Image.BILINEAR)\n","        lbl = lbl.resize(lbl_size, resample=Image.BICUBIC)\n","        \n","        img, lbl = ft.to_tensor(img), ft.to_tensor(lbl)\n","        mv = lbl.max(dim=1)[0].max(dim=1)[0]\n","        lbl = lbl / mv.unsqueeze(1).unsqueeze(1)\n","        # lbl = lbl.round()\n","        return img, lbl\n","\n","    def __len__(self):\n","      return len(self.filename)\n","\n","    def __getitem__(self, i):\n","        image = self.image[i]\n","        label = self.label[i]\n","        return self.augment(image, label)\n","\n","dataset = VertexSegmenterDataset('metadata.json')\n","len(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOB1334f5XjX","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(20,20))\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    index = random.randrange(len(dataset))\n","    img, lbl = dataset[index]\n","    plt.imshow(img.permute(1,2,0),\n","               extent=(0, 1, 0, 1))\n","    plt.imshow(lbl.permute(1,2,0),\n","               extent=(0, 1, 0, 1),\n","               alpha=0.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4EuDPfsaDZWI","colab_type":"text"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"TNmo8reX9JvD","colab_type":"code","colab":{}},"source":["class iblock(nn.Module):\n","    def __init__(self, dims):\n","        super(iblock, self).__init__()\n","        self.conv_path = nn.Sequential(nn.BatchNorm2d(dims), nn.GELU(),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1),\n","                                       nn.GELU(), nn.BatchNorm2d(dims),\n","                                       nn.Conv2d(dims, dims, kernel_size=3, padding=1))\n","        \n","    def forward(self, x):\n","        return x + self.conv_path(x)\n","\n","\n","def Pooling(in_dim, out_dim):\n","    return nn.Sequential(nn.BatchNorm2d(in_dim), nn.GELU(),\n","                         nn.Conv2d(in_dim, out_dim, kernel_size=2, stride=2))\n","\n","\n","class Segmenter(nn.Module):\n","    def __init__(self):\n","        super(Segmenter, self).__init__()\n","        self.downscale = nn.ModuleList([Pooling(8, 16), Pooling(16, 32), Pooling(32, 64), Pooling(64, 96)])\n","        self.upscale = nn.ModuleList([nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                                      nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)])\n","        \n","        self.pre_cnn = nn.Sequential(nn.Conv2d(3, 8, kernel_size=2, stride=2))\n","\n","        self.in_cnn = nn.ModuleList([nn.Sequential(iblock( 8), iblock( 8)),\n","                                     nn.Sequential(iblock(16), iblock(16)),\n","                                     nn.Sequential(iblock(32), iblock(32)),\n","                                     nn.Sequential(iblock(64), iblock(64))\n","                                     ])\n","\n","        self.bottom = nn.Sequential(iblock(96), iblock(96),\n","                                    nn.BatchNorm2d(96), nn.GELU(),\n","                                    nn.Conv2d(96, 64, kernel_size=1))\n","\n","        self.out_cnn = nn.ModuleList([nn.Sequential(iblock(64), iblock(64),\n","                                                    nn.GELU(), nn.BatchNorm2d(64),\n","                                                    nn.Conv2d(64, 32, kernel_size=1)),\n","                                      nn.Sequential(iblock(32), iblock(32),\n","                                                    nn.GELU(), nn.BatchNorm2d(32),\n","                                                    nn.Conv2d(32, 16, kernel_size=1)),\n","                                      nn.Sequential(iblock(16), iblock(16),\n","                                                    nn.GELU(), nn.BatchNorm2d(16),\n","                                                    nn.Conv2d(16, 8, kernel_size=1)),\n","                                      nn.Sequential(iblock(8), iblock(8),\n","                                                    nn.GELU(), nn.BatchNorm2d(8))\n","        ])\n","            \n","        self.last_cnn = nn.Sequential(nn.Conv2d(8, 3, kernel_size=1), nn.Sigmoid())\n","\n","    def forward(self, x):\n","        x = self.pre_cnn(x)\n","        mid = []\n","        for i in range(len(self.in_cnn)):\n","            x = self.in_cnn[i](x)\n","            mid.append(x)\n","            x = self.downscale[i](x)\n","        \n","        x = self.bottom(x)\n","\n","        for i in range(len(self.out_cnn)):\n","            x = self.out_cnn[i]( self.upscale[i](x) + mid.pop() )\n","        x = self.last_cnn(x)\n","        return x\n","\n","    def load(self, fname):\n","        self.load_state_dict(tr.load(fname, map_location=lambda storage, loc: storage))\n","\n","    def save(self, fname):\n","        tr.save(self.state_dict(), fname)\n","\n","npar = sum(p.numel() for p in Segmenter().parameters())\n","print(f\"{npar} parameters\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvhqWHT1Czgf","colab_type":"code","colab":{}},"source":["class DiceLoss(tr.nn.Module):\n","    def __init__(self, layer_weights):\n","        super(DiceLoss, self).__init__()\n","        self.layer_weights = layer_weights\n","        self.smooth = 1e-6\n","\n","    def forward(self, pred, target):\n","        intersection = (pred * target).sum(2).sum(2)\n","        sum_A = pred.sum(2).sum(2)\n","        sum_B = target.sum(2).sum(2)\n","        loss = 1 - (2 * intersection + self.smooth) / (sum_A + sum_B + self.smooth)\n","        loss = 100 * loss.mean(0)\n","        return loss, loss * self.layer_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPyi7uv0eoMe","colab_type":"code","colab":{}},"source":["class IoTLoss(tr.nn.Module):\n","    def __init__(self, layer_weights):\n","        super(IoTLoss, self).__init__()\n","        self.layer_weights = layer_weights\n","        self.smooth = 1e-6\n","\n","    def forward(self, pred, target):\n","        intersection = (pred * target).sum(2).sum(2)\n","        union = tr.max(pred, target).sum(2).sum(2)\n","        loss = 1 - (intersection + self.smooth) / (union + self.smooth)\n","        loss = 100 * loss.mean(0)\n","        return loss, loss * self.layer_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0P62IQUtFwE","colab_type":"code","colab":{}},"source":["class WBCELoss(tr.nn.Module):\n","    def __init__(self, layer_weights):\n","        super(WBCELoss, self).__init__()\n","        self.layer_weights = layer_weights\n","        self.bce = BCELoss(reduction='none')\n","\n","    def forward(self, pred, target):\n","        loss = 100 * self.bce(pred, target).mean(2).mean(2).mean(0)\n","        return loss, loss * self.layer_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQFtgrVqDbYV","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"7tUwnJ4Xjrl9","colab_type":"code","colab":{}},"source":["model = Segmenter().cuda()\n","\n","tr.manual_seed(42)\n","np.random.seed(42)\n","train, test = random_split(dataset, (len(dataset)-32, 32))\n","\n","fname ='segmenter_18'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsKBze-cDdg1","colab_type":"code","colab":{}},"source":["hist = hl.History()\n","canvas = hl.Canvas()\n","\n","epoch = 0\n","best_test_corner = float('inf')\n","best_test_total = float('inf')\n","\n","if os.path.isfile(\"models/\" + fname + \".pmt\"):\n","    model.load(\"models/\" + fname + \".pmt\")\n","    hist.load(\"models/\" + fname + \".hist\")\n","    epoch = len(hist['best_total'].data) + 1\n","    best_test_total = min(hist['best_total'].data)\n","    best_test_corner = min(hist['best_corner'].data)\n","\n","train_loader = DataLoader(train, batch_size=4, shuffle=True, num_workers=2)\n","test_loader = DataLoader(test, batch_size=4, shuffle=True, num_workers=2)\n","        \n","test_loss_func = IoTLoss(tr.Tensor([0.1, 0.5, 1.0]).cuda())\n","\n","epochs_without_improvement = 0\n","while epochs_without_improvement < 300:\n","    if epoch == 0:\n","        loss_func = WBCELoss(tr.Tensor([1.0, 1.0, 1.0]).cuda())\n","        optimizer = tr.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n","        scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-9)\n","    if epoch == 50:\n","        loss_func = IoTLoss(tr.Tensor([1.0, 1.0, 1.0]).cuda())\n","        optimizer = tr.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n","        scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-9)\n","    if epoch == 200:\n","        loss_func = IoTLoss(tr.Tensor([0.1, 0.1, 1.0]).cuda())\n","        optimizer = tr.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n","        scheduler = tr.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, min_lr=1e-9)\n","\n","    model.train()\n","    train_tot_loss = 0\n","    train_cor_loss = 0\n","    for img, lbl in train_loader:\n","        img, lbl = img.cuda(), lbl.cuda()\n","        pred = model(img)\n","        \n","        optimizer.zero_grad()\n","        loss, wloss = loss_func(pred, lbl)\n","\n","        wloss.mean().backward()\n","        optimizer.step()\n","        \n","        train_tot_loss += loss.mean().data.item() / len(train_loader)\n","        train_cor_loss += loss[2].data.item() / len(train_loader)\n","\n","    model.eval()\n","    test_tot_loss = 0\n","    test_cor_loss = 0\n","    for img, lbl in test_loader:\n","        img, lbl = img.cuda(), lbl.cuda()\n","        pred = model(img)\n","        loss, _ = test_loss_func(pred, lbl)\n","\n","        test_tot_loss += loss.mean().data.item() / len(test_loader)\n","        test_cor_loss += loss[2].data.item() / len(test_loader)\n","\n","    scheduler.step(test_cor_loss)\n","    if test_cor_loss < best_test_corner:\n","        model.save(\"models/\" + fname + \".pmt\")\n","        hist.save(\"models/\" + fname + \".hist\")\n","        best_test_corner = test_cor_loss\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","    best_test_total = min(test_tot_loss, best_test_total)\n","        \n","    if epoch > 0:\n","        hist.log(epoch, train_total = train_tot_loss,\n","                        train_corner= train_cor_loss,\n","                        best_total = best_test_total,\n","                        test_total = test_tot_loss,\n","                        test_corner= test_cor_loss,\n","                        best_corner= best_test_corner)\n","\n","        with canvas:\n","            canvas.draw_plot([hist[\"train_total\"],\n","                              hist[\"test_total\"],\n","                              hist[\"best_total\"]])\n","            canvas.draw_plot([hist[\"train_corner\"],\n","                              hist[\"test_corner\"],\n","                              hist[\"best_corner\"]])\n","    epoch += 1\n","\n","hist.summary()\n","hist.save(\"models/\" + fname + \".hist\")\n","model.load(\"models/\" + fname + \".pmt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKOjX6gnRqFX","colab_type":"text"},"source":["Baseline: 55.8404\n","\n","GELU: 54.5560\n","\n","Deeper: 53.4471\n","\n","Position dependant: 54.3545\n","\n","Hard sample minning:"]},{"cell_type":"markdown","metadata":{"id":"j6_N91h0Rxu5","colab_type":"text"},"source":["# Test\n"]},{"cell_type":"code","metadata":{"id":"JfZN4bIkoUjD","colab_type":"code","colab":{}},"source":["params_path = \"models/\" + fname + \".pmt\"\n","\n","class CornerDetector():\n","    def __init__(self, gpu=False):\n","        self.unet = Segmenter()\n","        self.unet.load(params_path)\n","        self.unet.eval()\n","        if gpu: \n","            self.unet = self.unet.cuda()\n","        self.gpu = gpu\n","\n","    def segment(self, image):\n","        tensor = ft.to_tensor(image).unsqueeze(0)\n","        if self.gpu:\n","            tensor = tensor.cuda()\n","        segmentation = self.unet(tensor)\n","        segmentation = segmentation.detach().cpu().numpy().squeeze()\n","        segmentation[segmentation < 0.1] = 0.0\n","        return segmentation\n","\n","    def detect_corner(self, image, seg):\n","        segmentation = np.copy(seg)\n","        ccomponent, ncomponent = ndimage.label(segmentation[0])\n","        greather_component = stats.mode(ccomponent[ccomponent>0], axis=None)[0]\n","        segmentation = segmentation[2]\n","        segmentation[ccomponent != greather_component] = 0.0\n","        \n","        ccomponent, ncomponent = ndimage.label(segmentation)\n","        if ncomponent < 4: raise Exception(f\"Missing {4 - ncomponent} corners.\")\n","\n","        confidence = np.zeros((4))\n","        vertexs = -np.ones((4, 2))\n","        for i in range(4):\n","            max_probability = segmentation.max()\n","            confidence[i] = max_probability\n","            max_position = np.where(segmentation == max_probability)\n","\n","            mask = ccomponent[max_position[0][0], max_position[1][0]] == ccomponent\n","            p = np.where(mask)\n","            w = segmentation[mask]\n","            w /= w.sum()\n","            vertexs[i] = np.array([(p[0] * w).sum(), (p[1] * w).sum()])\n","            segmentation[mask] = 0.0\n","        vertexs = 2 * vertexs[:,[1,0]]\n","        idxs = self.order_vertexs(vertexs, image.size)\n","        return vertexs[idxs]\n","\n","    def order_vertexs(self, v, img_size):\n","        w, h = img_size\n","        vc = v.copy()\n","        idxs = np.ones(4).astype(int)\n","        idxs[0] = np.linalg.norm(vc, ord=2, axis=1).argmin()\n","        vc[idxs[0]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[1] = np.linalg.norm(vc - np.array([w,0]), ord=2, axis=1).argmin()\n","        vc[idxs[1]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[2] = np.linalg.norm(vc - np.array([w,h]), ord=2, axis=1).argmin()\n","        vc[idxs[2]] = np.array([float('inf'), float('inf')])\n","\n","        idxs[3] = np.linalg.norm(vc - np.array([0,h]), ord=2, axis=1).argmin()\n","        vc[idxs[3]] = np.array([float('inf'), float('inf')])\n","\n","        last_prod = 0\n","        for i in range(len(v)):\n","            prev = v[idxs[(i-1)%4]] - v[idxs[i]]\n","            post = v[idxs[(i+1)%4]] - v[idxs[i]]\n","            cross_prod = np.cross(post, prev)\n","            if cross_prod * last_prod < 0:\n","                idxs[i], idxs[(i+1)%4] = idxs[(i+1)%4].copy(), idxs[i].copy()\n","                i += 1\n","            else:\n","                last_prod = cross_prod\n","        return idxs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhK68bv4-u8h","colab_type":"code","colab":{}},"source":["vertex_detector = CornerDetector(gpu=False)\n","\n","tr.manual_seed(42)\n","np.random.seed(42)\n","_, test = random_split(dataset, (len(dataset)-32, 32))\n","\n","plt.figure(figsize=(20,40))\n","for i in range(len(test)):\n","    plt.subplot(8, 4, i+1)\n","    img, _ = test[i]\n","    img = ft.to_pil_image(img)\n","\n","    segmentation = vertex_detector.segment(img)\n","    try:\n","        pred_vertex = vertex_detector.detect_corner(img, segmentation)\n","    except:\n","        print('Corner missing!')\n","        continue\n","    draw = ImageDraw.Draw(img)\n","    draw.polygon([(pred_vertex[i][0], pred_vertex[i][1]) for i in range(4)])\n","    del draw\n","    plt.imshow(img, extent = (0,1,0,1))\n","    plt.imshow(segmentation[2],\n","                cmap='jet',\n","                norm=Normalize(0, 1),\n","                extent = (0,1,0,1),\n","                alpha=0.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5O6L_ojn214","colab_type":"code","colab":{}},"source":["vertex_detector = CornerDetector(gpu=False)\n","\n","nplots = 0\n","dist = []\n","plt.figure(figsize=(20,40))\n","nmc = 0\n","for i in tqdm(range(len(dataset))):\n","    img = dataset.image[i]\n","    true_vertex = np.array(dataset.corner_coords[i])\n","    segmentation = vertex_detector.segment(img)\n","    fail = False\n","    try:\n","        pred_vertex = vertex_detector.detect_corner(img, segmentation)\n","    except:\n","        fail = True\n","        nmc += 1\n","        continue\n","\n","    true_vertex[:,0] *= 512 / 100\n","    true_vertex[:,1] *= 384 / 100\n","    \n","    d = np.sqrt(((pred_vertex - true_vertex)**2).sum(1))\n","    dist.append(d)\n","\n","    if (d > 30).any() or fail:\n","        plt.subplot(5, 4, nplots+1)\n","        nplots += 1\n","        draw = ImageDraw.Draw(img)\n","        draw.polygon([(pred_vertex[i][0], pred_vertex[i][1]) for i in range(4)])\n","        del draw\n","        plt.imshow(img, extent = (0,1,0,1))\n","        plt.imshow(segmentation[2],\n","                    cmap='jet',\n","                    norm=Normalize(0, 1),\n","                    extent = (0,1,0,1),\n","                    alpha=0.5)\n","        plt.title(dataset.filename[i])\n","print(f\"Missing corners: {nmc}\")\n","dist = np.concatenate(dist)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkMBYWOAWNt2","colab_type":"code","colab":{}},"source":["dist[dist<30].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRo9uoRnX7kh","colab_type":"code","colab":{}},"source":["np.histogram(dist[dist<30])"],"execution_count":null,"outputs":[]}]}